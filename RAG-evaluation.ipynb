{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Retrieval Augmented Generation modification and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: httpx verion 0.27.0  is necessary to use the httpx.AsyncClient with groq. langchain issue that needs fixing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import Dict\n",
    "from pathlib import Path  # For working with file paths\n",
    "\n",
    "# Utility libraries\n",
    "from dotenv import load_dotenv  # For loading environment variables from a .env file\n",
    "import glob  # For matching file paths using patterns\n",
    "import tqdm  # For displaying progress bars in loops\n",
    "import pandas as pd  # For handling tabular data\n",
    "from datasets import Dataset  # For managing datasets (Hugging Face)\n",
    "\n",
    "# PDF handling\n",
    "from PyPDF2 import PdfReader  # For extracting text from PDF files\n",
    "\n",
    "# LangChain core functionality\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # For splitting text into manageable chunks\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate  # For defining and managing prompt templates\n",
    "from langchain.vectorstores import Chroma  # For creating vector stores for retrieval\n",
    "from langchain.embeddings import HuggingFaceEmbeddings  # For generating embeddings for text chunks\n",
    "\n",
    "# LangChain advanced components\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain  # For combining retrieved documents\n",
    "from langchain_core.output_parsers import StrOutputParser  # For parsing string outputs from models\n",
    "\n",
    "# Third-party AI model interfaces\n",
    "from langchain_openai import ChatOpenAI  # For using OpenAI models with LangChain\n",
    "from langchain_groq import ChatGroq  # For using Groq models with LangChain\n",
    "import openai  # For using OpenAI's API\n",
    "\n",
    "# For displaying notebook progress bars\n",
    "import tqdm\n",
    "import tqdm.notebook as notebook_tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bugfix for Chroma with SQLite\n",
    "\n",
    "This cell addresses a known issue with Chroma's dependency on `sqlite3`, which can conflict with certain Python environments. \n",
    "\n",
    "#### What this does:\n",
    "1. **Replaces `sqlite3` with `pysqlite3`:**\n",
    "   - Ensures compatibility by importing `pysqlite3` as a substitute for `sqlite3`.\n",
    "   - Updates the `sys.modules` mapping to ensure all imports of `sqlite3` use `pysqlite3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd()\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "DATABASES = {\n",
    "    'default': {\n",
    "        'ENGINE': 'django.db.backends.sqlite3',\n",
    "        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Environment Variables\n",
    "\n",
    "This cell loads sensitive environment variables such as API keys from a `.env` file. Using environment variables helps keep credentials secure and out of the source code.\n",
    "\n",
    "#### What this does:\n",
    "1. **`load_dotenv()`:**\n",
    "   - Loads environment variables from a `.env` file in the current working directory.\n",
    "   - A `.env` file typically contains key-value pairs (e.g., `GROQ_API_KEY=your_groq_api_key`).\n",
    "\n",
    "2. **Retrieve API Keys:**\n",
    "   - `os.getenv(\"GROQ_API_KEY\")`: Retrieves the GROQ API key.\n",
    "   - `os.getenv(\"OPENAI_API_KEY\")`: Retrieves the OpenAI API key.\n",
    "\n",
    "\n",
    "#### Notes:\n",
    "- Ensure you have a `.env` file in the root of your project directory with the required keys, for example:\n",
    "  ```plaintext\n",
    "  GROQ_API_KEY=your_groq_api_key\n",
    "  OPENAI_API_KEY=your_openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Load and Extract Text from PDFs\n",
    "\n",
    "In this task, you will load multiple PDF files from a specified directory, read their content, and extract text. This text will later be used for processing and retrieval.\n",
    "\n",
    "#### Instructions:\n",
    "1. **Define the File Path:**\n",
    "\n",
    "2. **Iterate Over PDFs:**\n",
    "   - Use the `glob` library to find all files matching the `*.pdf` pattern in the specified directory.\n",
    "   - For each file, open it in binary mode (`\"rb\"`) using a `with` statement.\n",
    "\n",
    "3. **Extract Text:**\n",
    "   - Use the `PdfReader` library to read the PDF content.\n",
    "   - Iterate through the pages and extract text from each page.\n",
    "\n",
    "4. **Combine Text:**\n",
    "   - Concatenate the text from all pages into a single string (`text`).\n",
    "\n",
    "5. **Preview the Output:**\n",
    "   - Print the first 50 characters of the extracted text to verify that the content is loaded correctly.\n",
    "\n",
    "#### What to Do:\n",
    "- Run the cell and inspect the first 50 characters of the extracted text to confirm it works as expected.\n",
    "- If necessary, adjust the `glob_path` to point to the correct directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_path = \"\"\n",
    "\n",
    "\n",
    "text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Split Extracted Text into Manageable Chunks\n",
    "\n",
    "#### Instructions:\n",
    "1. **Create a Text Splitter:**\n",
    "   - Use the `RecursiveCharacterTextSplitter` to split the text.\n",
    "   - Specify two key parameters:\n",
    "     - **`chunk_size` (2000):** The maximum number of characters in each chunk.\n",
    "     - **`chunk_overlap` (200):** The number of overlapping characters between consecutive chunks to maintain context continuity\n",
    "2. **Inspect the Chunks:**\n",
    "   - After splitting, verify the output by inspecting the `chunks` variable. Each chunk should be approximately 2000 characters long, with overlaps of 200 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chunks))\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Create Embeddings for Text Chunks\n",
    "\n",
    "In this step, you will initialize the embedding model that will convert the text chunks into numerical representations (embeddings). These embeddings are essential for enabling similarity-based retrieval in the RAG system.\n",
    "\n",
    "#### Instructions:\n",
    "1. **Select an Embedding Model:**\n",
    "   - Use the `HuggingFaceEmbeddings` class to specify the embedding model.\n",
    "   - The model name provided here is `\"sentence-transformers/all-mpnet-base-v2\"`, a widely used embedding model for generating high-quality text representations.\n",
    "\n",
    "2. **Initialize the Model:**\n",
    "   - Pass the model name as an argument to `HuggingFaceEmbeddings` and assign the resulting object to the variable `embeddings`.\n",
    "\n",
    "\n",
    "#### What to Do:\n",
    "- Use the `HuggingFaceEmbeddings` class to load the specified embedding model.\n",
    "- Assign the loaded model to the `embeddings` variable.\n",
    "\n",
    "#### Documentation\n",
    "https://python.langchain.com/api_reference/huggingface/embeddings/langchain_huggingface.embeddings.huggingface.HuggingFaceEmbeddings.html#huggingfaceembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Create a Vector Store for Text Retrieval\n",
    "\n",
    "\n",
    "#### Instructions:\n",
    "1. **Generate the Vector Store:**\n",
    "   - Use the `Chroma.from_texts` method to create a vector store.\n",
    "   - Pass the `chunks` (text chunks) and the `embeddings` object (created in the previous step) as arguments.\n",
    "\n",
    "\n",
    "3. **Inspect the Output:**\n",
    "   - Optionally, inspect the `vector_store` to confirm that it is ready for retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Create a Retriever for the Vector Store\n",
    "\n",
    "In this step, you will create a retriever to query the vector store and fetch the most relevant text chunks for a given input query. The retriever uses the vector embeddings to perform similarity-based searches.\n",
    "\n",
    "#### Instructions:\n",
    "1. **Create the Retriever:**\n",
    "   - Use the `as_retriever` method on the `vector_store` to create a retriever.\n",
    "   - Set the `search_type` parameter to `\"mmr\"` (Maximal Marginal Relevance) to ensure diverse and relevant retrieval.\n",
    "   - Pass additional search settings using `search_kwargs`, such as:\n",
    "     - **`k`:** The number of chunks to retrieve (e.g., `k=3`).\n",
    "\n",
    "2. **Assign the Retriever:**\n",
    "   - Store the retriever in the variable `retriever` for later use in querying the vector store.\n",
    "\n",
    "3. **Verify the Retriever:**\n",
    "   - Ensure the retriever is correctly initialized and ready to handle queries.\n",
    "\n",
    "\n",
    "https://python.langchain.com/docs/integrations/vectorstores/chroma/#query-by-turning-into-retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(\"How do I diagnose Asthma?\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Build a RAG Model Function\n",
    "\n",
    "In this task, you will combine all the steps from the previous tasks to create a reusable function for building a Retrieval-Augmented Generation (RAG) model. The function will process raw text documents, generate embeddings, and store them in a vector store for efficient retrieval.\n",
    "\n",
    "#### What You Need to Do:\n",
    "\n",
    "1. **Define the Function:**\n",
    "   - Create a function named `build_rag_model` with the following parameters:\n",
    "     - **`texts` (List[str]):** A list of raw documents or text strings to process.\n",
    "     - **`embedding_model` (str):** The name of the Hugging Face embedding model to use.\n",
    "     - **`chunk_size` (int):** The maximum size of each text chunk.\n",
    "     - **`chunk_overlap` (int):** The overlap size between consecutive chunks.\n",
    "\n",
    "2. **Implement the Steps:**\n",
    "   - **Step 1:** **Split Text into Chunks**\n",
    "     - Use `RecursiveCharacterTextSplitter` to split the provided `texts` into chunks.\n",
    "     - Ensure the function handles all documents in the list and combines the resulting chunks.\n",
    "     - Print the number of generated chunks for debugging purposes.\n",
    "\n",
    "   - **Step 2:** **Generate Embeddings**\n",
    "     - Initialize a `HuggingFaceEmbeddings` object using the provided `embedding_model`.\n",
    "     - Use this object to generate embeddings for the text chunks.\n",
    "\n",
    "   - **Step 3:** **Create a Vector Store**\n",
    "     - Use `Chroma.from_texts` to create a vector store from the chunks and their embeddings.\n",
    "     - Print the number of chunks stored in the vector store for confirmation.\n",
    "\n",
    "3. **Return the Vector Store:**\n",
    "   - The function should return the vector store so it can be used for retrieval tasks.\n",
    "\n",
    "\n",
    "#### Example Usage:\n",
    "- Call the function like this:\n",
    "  ```python\n",
    "  retriever = build_rag_model(\n",
    "      texts=[\"Asthma is a chronic condition.\", \"Hypertension is persistently high blood pressure.\"],\n",
    "      embedding_model=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "      chunk_size=200,\n",
    "      chunk_overlap=50\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_model(texts, embedding_model, chunk_value):\n",
    "    \"\"\"\n",
    "    ADD LOGIC HERE\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"Generated {len(chunks)} chunks from {len(texts)} documents.\")\n",
    "\n",
    "    # Step 2: Generate embeddings\n",
    "    \n",
    "    \n",
    "    # Step 3: Create vector store\n",
    "    \n",
    "    \n",
    "    print(f\"Vector store created with {len(chunks)} chunks.\")\n",
    "    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Define a Generative Model for Question Answering\n",
    "\n",
    "In this task, you will define and initialize a generative model that can answer user questions based on a given context. This involves creating a prompt template, setting up an output parser, and initializing the language model for generation.\n",
    "\n",
    "#### What You Need to Do:\n",
    "\n",
    "1. **Define the Prompt Template:**\n",
    "   - Use the `PromptTemplate` class to define a template that specifies how user questions and the associated context are structured.\n",
    "   - Your template should:\n",
    "     - Include placeholders for the context (`{context}`) and question (`{question}`).\n",
    "     - Provide clear instructions for the model to generate answers based only on the context.\n",
    "\n",
    "2. **Initialize the Prompt Template:**\n",
    "   - Set the `template` argument to the system template:\n",
    "   - Specify the `input_variables` as `[\"context\", \"question\"]` to define the placeholders.\n",
    "\n",
    "3. **Set Up the Output Parser:**\n",
    "   - Use the `StrOutputParser` to parse the string output from the model.\n",
    "\n",
    "4. **Initialize the Generative Model:**\n",
    "   - Use the `ChatGroq` class to set up a generative model with the following parameters:\n",
    "     - **`model`:** Specify the model name (e.g., `\"llama-3.2-3b-preview\"`).\n",
    "     - **`temperature`:** Set to `0` for deterministic outputs.\n",
    "     - **`max_tokens`:** Set to `None` to allow the model to decide the output length.\n",
    "     - **`timeout`:** Set to handle timeouts during generation.\n",
    "     - **`max_retries`:** Define the number of retries in case of failure.\n",
    "\n",
    "\n",
    "Groq documentation: https://python.langchain.com/v0.1/docs/integrations/chat/groq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the template for answering user questions based on a provided context\n",
    "system_template = \"\"\"\n",
    "<context> {context} </context>\n",
    "<question> {question} </question>\n",
    "\"\"\"\n",
    "# Create a prompt template for the question-answering system\n",
    "\n",
    "# Initialize the generative model for question answering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Build the RAG Chain for Question Answering\n",
    "\n",
    "In this task, you will create a **RAG (Retrieval-Augmented Generation) Chain** that connects the components youâ€™ve defined so far: the prompt template, the generative model, and the output parser. This chain orchestrates the process of answering user questions by sequentially formatting inputs, generating answers, and parsing outputs.\n",
    "\n",
    "#### What You Need to Do:\n",
    "\n",
    "1. **Chain the Components:**\n",
    "   - Use the pipe operator (`|`) to sequentially combine the components:\n",
    "     - **`question_answering_prompt`:** Formats the user question and context into the structured template.\n",
    "     - **`model`:** The generative model processes the formatted input and generates a response.\n",
    "     - **`output_parser`:** Parses the raw response from the model into a structured and usable format.\n",
    "\n",
    "2. **Assign the Chain:**\n",
    "   - Store the combined components into the variable `rag_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag_chain = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test your chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do I diagnose Asthma?\"\n",
    "print(rag_chain.invoke({\"context\": docs, \"question\": query}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Define a Function to Answer Questions Using RAG\n",
    "\n",
    "In this task, you will create a function that leverages the RAG (Retrieval-Augmented Generation) system to answer user questions. The function will retrieve relevant documents from the knowledge index and use the RAG chain to generate a response.\n",
    "\n",
    "#### What You Need to Do:\n",
    "\n",
    "1. **Define the Function:**\n",
    "   - Name the function `answer_with_rag`.\n",
    "   - Specify the following arguments:\n",
    "     - **`question` (str):** The user's query.\n",
    "     - **`rag_chain`:** The RAG chain you built earlier for formatting, generating, and parsing responses.\n",
    "     - **`retriever` (VectorStore):** The vector store containing document embeddings for retrieval.\n",
    "\n",
    "2. **Implement the Steps:**\n",
    "   - **Step 1:** Retrieve Relevant Documents\n",
    "     - Use the `retriever` to retrieve documents related to the query.\n",
    "\n",
    "   - **Step 2:** Prepare the Input for the RAG Chain\n",
    "     - Create a dictionary named `rag_input` with the following keys:\n",
    "       - **`context`:** A list of retrieved document texts.\n",
    "       - **`question`:** The user query.\n",
    "\n",
    "   - **Step 3:** Generate an Answer\n",
    "     - Pass the `rag_input` to the `rag_chain` using the `invoke` method.\n",
    "     - Store the generated response in the variable `answer`.\n",
    "\n",
    "3. **Return the Results:**\n",
    "   - The function should return a tuple containing:\n",
    "     - **`answer` (str):** The generated response to the question.\n",
    "     - **`relevant_docs` (List[str]):** The list of retrieved document texts used for answering the query.\n",
    "\n",
    "4. **Test the Function:**\n",
    "   - Test the function with sample questions and ensure it retrieves relevant documents and generates accurate answers.\n",
    "\n",
    "#### Example Usage:\n",
    "- Call the function like this:\n",
    "  ```python\n",
    "  answer, relevant_docs = answer_with_rag(\n",
    "      question=\"What are the symptoms of asthma?\",\n",
    "      rag_chain=rag_chain,  # Your defined RAG chain\n",
    "      knowledge_index=knowledge_index  # Your vector store retriever\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_rag(\n",
    "    question,\n",
    "    rag_chain,  \n",
    "    retriever):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "\n",
    "    # Limit to the top N final documents\n",
    "\n",
    "    # Pass the documents and the question to the RAG chain\n",
    "  \n",
    "\n",
    "    # Use the RAG chain to generate an answer\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_with_rag(\"what is asthma?\", rag_chain, retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Set Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation: Function to Call the OpenAI API\n",
    "\n",
    "This function interacts with the OpenAI API to generate responses based on a given prompt. It provides a simple wrapper for querying the API and returning the generated output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call the OpenAI API\n",
    "def call_llm(prompt: str):\n",
    "    \"\"\"\n",
    "    Calls the OpenAI API to generate a response for a given prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt for the LLM.\n",
    "        model (str): The OpenAI model to use (default is \"gpt-4\").\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response from the LLM.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: Prompt for QA Generation\n",
    "\n",
    "This prompt template defines the instructions for generating factoid-style question-answer (QA) pairs based on a given context. It is specifically crafted to create search-engine-style questions and concise, factual answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Question-Answer (QA) Pairs\n",
    "\n",
    "1. **Set the Number of QA Pairs to Generate:**\n",
    "   - **`N_GENERATIONS`:** Specifies the maximum number of QA pairs to generate. Here, it is set to `30`.\n",
    "\n",
    "2. **Sample Chunks:**\n",
    "   - Randomly selects `N_GENERATIONS` chunks from the `chunks` using `random.sample`.\n",
    "\n",
    "3. **Loop Over Chunks:**\n",
    "   - For each sampled chunk:\n",
    "     - **Step 1:** Format the prompt:\n",
    "       - Replaces the `{context}` placeholder in `QA_generation_prompt` with the text of the current chunk.\n",
    "     - **Step 2:** Call the LLM:\n",
    "       - Sends the formatted prompt to the `call_llm` function to generate a question and its corresponding answer.\n",
    "     - **Step 3:** Extract Question and Answer:\n",
    "       - Parses the output to extract the `Factoid question` and `Answer` fields.\n",
    "     - **Step 4:** Validate and Append:\n",
    "       - Ensures the answer is less than 300 characters long.\n",
    "       - Appends the valid `context`, `question`, and `answer` to the `outputs` list.\n",
    "\n",
    "4. **Handle Errors:**\n",
    "   - If an error occurs during QA generation (e.g., malformed output), it skips the current chunk and logs the error.\n",
    "\n",
    "5. **Display the Results:**\n",
    "   - After processing all chunks, prints the generated QA pairs for inspection.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Why This is Important:\n",
    "- This step generates a dataset of factoid-style QA pairs, which is essential for:\n",
    "  - Evaluating the RAG system's performance.\n",
    "  - Testing how well the QA pipeline retrieves relevant context and generates accurate answers.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example Output:\n",
    "- Each generated entry in `outputs` will look like this:\n",
    "  ```python\n",
    "  {\n",
    "      \"context\": \"Asthma is a chronic condition that affects the airways.\",\n",
    "      \"question\": \"What is asthma?\",\n",
    "      \"answer\": \"A chronic condition that affects the airways.\"\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_GENERATIONS = 30\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "# Generate QA pairs\n",
    "outputs = []\n",
    "for sampled_context in tqdm.tqdm(random.sample(chunks, min(N_GENERATIONS, len(chunks)))):\n",
    "    # Generate QA couple\n",
    "    try:\n",
    "        formatted_prompt = QA_generation_prompt.format(context=sampled_context)\n",
    "        output_QA_couple = call_llm(formatted_prompt)\n",
    "        # Extract question and answer from the output\n",
    "        question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0].strip()\n",
    "        answer = output_QA_couple.split(\"Answer: \")[-1].strip()\n",
    "        # Validate and append to outputs\n",
    "        assert len(answer) < 300, \"Answer is too long\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped a context due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Print generated outputs\n",
    "for output in outputs:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(outputs).head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Filtering with Critiques\n",
    "\n",
    "These prompts are designed to evaluate the quality of the generated factoid questions based on specific criteria: **groundedness**, **relevance**, and **stand-alone clarity**. Each prompt asks the LLM to provide a score and a rationale for the rating.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Groundedness Critique Prompt**\n",
    "\n",
    "##### Purpose:\n",
    "- To evaluate how well the question can be answered using the provided context.\n",
    "- Ensures the question is clearly and unambiguously grounded in the given text.\n",
    "\n",
    "##### Details:\n",
    "- The rating scale is from **1 to 5**:\n",
    "  - **1:** The question cannot be answered at all using the context.\n",
    "  - **5:** The question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "#### **2. Relevance Critique Prompt**\n",
    "\n",
    "##### Purpose:\n",
    "- To assess how useful the question is for developers, particularly in machine learning or NLP applications.\n",
    "- Ensures the question is aligned with the needs of the target audience (e.g., developers building with Hugging Face).\n",
    "\n",
    "##### Details:\n",
    "- The rating scale is from **1 to 5**:\n",
    "  - **1:** The question is not useful at all.\n",
    "  - **5:** The question is highly useful and relevant to the audience.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Stand-Alone Critique Prompt**\n",
    "\n",
    "##### Purpose:\n",
    "- To determine if the question can be understood without additional context.\n",
    "- Ensures the question is self-contained and meaningful to someone with domain knowledge or access to related documentation.\n",
    "\n",
    "##### Details:\n",
    "- The rating scale is from **1 to 5**:\n",
    "  - **1:** The question depends on additional information (e.g., \"in the context\" or \"in the document\").\n",
    "  - **5:** The question is fully understandable and stand-alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critique QA Pairs Using LLM Prompts\n",
    "\n",
    "In this task, you will evaluate each generated QA pair using the previously defined critique prompts for **groundedness**, **relevance**, and **stand-alone clarity**. The goal is to score and document the quality of each question based on the provided context and criteria.\n",
    "\n",
    "---\n",
    "\n",
    "#### What This Code Does:\n",
    "\n",
    "1. **Iterate Over QA Outputs:**\n",
    "   - Loops through the `outputs` list, which contains the generated QA pairs (`context`, `question`, `answer`).\n",
    "\n",
    "2. **Generate Evaluations:**\n",
    "   - For each QA pair:\n",
    "     - **Groundedness:** Uses the `question_groundedness_critique_prompt` to evaluate if the question is answerable based on the given context.\n",
    "     - **Relevance:** Uses the `question_relevance_critique_prompt` to evaluate if the question is useful for the intended audience.\n",
    "     - **Stand-alone Clarity:** Uses the `question_standalone_critique_prompt` to evaluate if the question is understandable without additional context.\n",
    "\n",
    "3. **Call the LLM for Each Criterion:**\n",
    "   - Sends the formatted prompt for each criterion to the LLM using `call_llm`.\n",
    "   - Stores the response in the `evaluations` dictionary under the respective criterion.\n",
    "\n",
    "4. **Parse the Results:**\n",
    "   - Extracts the **`Total rating`** (score) and **`Evaluation`** (text rationale) from the LLM's response.\n",
    "   - Updates the `output` dictionary with the scores and evaluations for each criterion.\n",
    "\n",
    "5. **Handle Errors Gracefully:**\n",
    "   - If any part of the process fails (e.g., LLM output is malformed), the loop skips the current QA pair and continues with the next one.\n",
    "\n",
    "6. **Update Outputs:**\n",
    "   - Adds the critique scores and rationale to each QA pair in the `outputs` list.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example Output:\n",
    "\n",
    "Each `output` in the `outputs` list will be updated with fields like these:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"context\": \"Asthma is a chronic condition that affects the airways.\",\n",
    "    \"question\": \"What is asthma?\",\n",
    "    \"answer\": \"A chronic condition that affects the airways.\",\n",
    "    \"groundedness_score\": 5,\n",
    "    \"groundedness_eval\": \"The question is fully answerable based on the provided context.\",\n",
    "    \"relevance_score\": 4,\n",
    "    \"relevance_eval\": \"This question is relevant to an audience seeking general knowledge about asthma.\",\n",
    "    \"standalone_score\": 5,\n",
    "    \"standalone_eval\": \"The question is clear and understandable without additional context.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "for output in tqdm.tqdm(outputs):\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(\n",
    "            question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]),\n",
    "        ),\n",
    "        \"relevance\": call_llm(\n",
    "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "        \"standalone\": call_llm(\n",
    "            question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "    }\n",
    "    try:\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            score, eval = (\n",
    "                int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
    "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
    "            )\n",
    "            output.update(\n",
    "                {\n",
    "                    f\"{criterion}_score\": score,\n",
    "                    f\"{criterion}_eval\": eval,\n",
    "                }\n",
    "            )\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation: Filtering and Preparing the Evaluation Dataset\n",
    "\n",
    "In this step, we transform the evaluated QA pairs into a structured dataset, filter them based on their scores, and prepare the final dataset for further evaluation or model training.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step-by-Step Breakdown:\n",
    "\n",
    "\n",
    "2. **Convert QA Pairs to a DataFrame:**\n",
    "   - `generated_questions = pd.DataFrame.from_dict(outputs)`:\n",
    "     - Converts the `outputs` list (which now includes QA pairs and their scores) into a pandas DataFrame for easier manipulation and analysis.\n",
    "\n",
    "3. **Display the Evaluation Dataset (Before Filtering):**\n",
    "   - Prints a subset of columns:\n",
    "     - **`question`:** The generated question.\n",
    "     - **`answer`:** The corresponding answer.\n",
    "     - **`groundedness_score`, `relevance_score`, `standalone_score`:** Scores assigned during the critique step.\n",
    "   - This provides an overview of the dataset before applying any filtering criteria.\n",
    "\n",
    "4. **Filter the QA Pairs:**\n",
    "   - Keeps only QA pairs that meet the following conditions:\n",
    "     - **`groundedness_score` >= 4:** The question is well-anchored in the provided context.\n",
    "     - **`standalone_score` >= 4:** The question is clear and understandable without additional context.\n",
    "   - **Note:** The `relevance_score` is not used for filtering here, but it remains part of the dataset for reference.\n",
    "\n",
    "5. **Display the Filtered Dataset:**\n",
    "   - Prints the filtered DataFrame to show the high-quality QA pairs that passed the criteria.\n",
    "\n",
    "6. **Convert to a Hugging Face Dataset:**\n",
    "   - `eval_dataset = datasets.Dataset.from_pandas(generated_questions, split=\"train\", preserve_index=False)`:\n",
    "     - Converts the filtered pandas DataFrame into a Hugging Face `Dataset` object, which is commonly used for training and evaluation in NLP tasks.\n",
    "     - The `split=\"train\"` argument designates this as a training split.\n",
    "     - `preserve_index=False` ensures the index from the pandas DataFrame is not carried over to the `Dataset`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Purpose of This Step:\n",
    "\n",
    "1. **Dataset Refinement:**\n",
    "   - Filters out low-quality QA pairs to ensure only well-scored questions and answers are included in the final dataset.\n",
    "   - Focuses on groundedness and stand-alone clarity to improve the overall utility and reliability of the dataset.\n",
    "\n",
    "2. **Final Dataset Preparation:**\n",
    "   - Converts the data into a format suitable for further evaluation or training machine learning models, such as Hugging Face models.\n",
    "\n",
    "3. **Quality Assurance:**\n",
    "   - Provides a visual overview of the dataset before and after filtering, allowing for manual inspection of the data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "\n",
    "print(\"Evaluation dataset before filtering:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "generated_questions = generated_questions.loc[\n",
    "    (generated_questions[\"groundedness_score\"] >= 4)\n",
    "    & (generated_questions[\"standalone_score\"] >= 4)\n",
    "]\n",
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(generated_questions, split=\"train\", preserve_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation: Running RAG Tests\n",
    "\n",
    "This function evaluates the performance of the RAG (Retrieval-Augmented Generation) system by comparing the system's generated answers to the true answers in a test dataset. The results are saved to a file for further analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### What This Function Does:\n",
    "\n",
    "1. **Prepare the Output File:**\n",
    "   - Attempts to load existing test results from `output_file`:\n",
    "     - If the file exists, appends new results to the previous ones.\n",
    "     - If the file does not exist, initializes an empty `outputs` list.\n",
    "\n",
    "2. **Iterate Over the Evaluation Dataset:**\n",
    "   - Loops through the `eval_dataset`, which contains the test questions, true answers, and source documents.\n",
    "\n",
    "3. **Skip Already Evaluated Questions:**\n",
    "   - Checks if a question has already been tested (i.e., exists in the loaded `outputs`).\n",
    "   - Skips the question if it has already been evaluated.\n",
    "\n",
    "4. **Run the RAG System:**\n",
    "   - Calls the `answer_with_rag` function to:\n",
    "     - Retrieve relevant documents using the `knowledge_index`.\n",
    "     - Generate an answer using the LLM (Language Model).\n",
    "\n",
    "5. **Print Results (Optional):**\n",
    "   - If `verbose=True`, prints the following details for manual inspection:\n",
    "     - The input question.\n",
    "     - The generated answer.\n",
    "     - The true answer from the dataset.\n",
    "\n",
    "6. **Save Results:**\n",
    "   - Constructs a dictionary containing:\n",
    "     - The question and its true answer.\n",
    "     - The source document.\n",
    "     - The generated answer.\n",
    "     - The retrieved documents used to generate the answer.\n",
    "     - Test settings, if provided.\n",
    "   - Appends the result to the `outputs` list and saves it to the `output_file` in JSON format.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Example Usage:\n",
    "\n",
    "```python\n",
    "run_rag_tests(\n",
    "    eval_dataset=eval_dataset,  # Test dataset\n",
    "    llm=rag_chain,  # RAG chain (includes retrieval and generation)\n",
    "    knowledge_index=knowledge_index,  # Vector store retriever\n",
    "    output_file=\"rag_test_results.json\",  # File to save the results\n",
    "    verbose=True,  # Print results for inspection\n",
    "    test_settings={\n",
    "        \"embedding_model\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        \"chunk_size\": 200,\n",
    "        \"overlap\": 50,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset,\n",
    "    llm,\n",
    "    knowledge_index,\n",
    "    output_file: str,\n",
    "    verbose = True,\n",
    "    test_settings = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation: Setting Up the Evaluation Prompt\n",
    "\n",
    "This step defines the evaluation prompt that will be used to assess the quality of responses generated by the RAG system. The prompt follows a structured format to ensure consistent and objective evaluation based on a predefined scoring rubric.\n",
    "\n",
    "---\n",
    "\n",
    "#### Purpose of the Evaluation Prompt:\n",
    "\n",
    "1. **Define the Evaluation Task:**\n",
    "   - The LLM is tasked with comparing a generated response (`response`) to a reference answer (`reference_answer`) and scoring its quality based on specific criteria.\n",
    "\n",
    "2. **Provide a Score Rubric:**\n",
    "   - A detailed rubric is included to guide the LLM in assigning scores. The rubric ensures that scoring is based strictly on correctness, accuracy, and factual alignment with the reference answer.\n",
    "\n",
    "3. **Standardize Output:**\n",
    "   - The LLM is instructed to:\n",
    "     - Write a detailed feedback summary addressing the evaluation criteria.\n",
    "     - Assign a numerical score between 1 and 5, strictly adhering to the rubric.\n",
    "     - Format the output using the required structure, including `[RESULT]`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Structure of the Prompt:\n",
    "\n",
    "1. **Task Description:**\n",
    "   - Specifies the evaluation task and output format.\n",
    "   - Emphasizes that the feedback must focus on the score rubric and avoid general evaluations.\n",
    "\n",
    "2. **Instruction to Evaluate:**\n",
    "   - The instruction or context that prompted the response.\n",
    "\n",
    "3. **Response to Evaluate:**\n",
    "   - The generated response being evaluated.\n",
    "\n",
    "4. **Reference Answer:**\n",
    "   - The ideal answer that would receive a perfect score of 5.\n",
    "\n",
    "5. **Score Rubrics:**\n",
    "   - Provides explicit criteria for scoring:\n",
    "     - **Score 1:** Completely incorrect and inaccurate.\n",
    "     - **Score 5:** Completely correct, accurate, and factual.\n",
    "\n",
    "6. **Feedback Section:**\n",
    "   - Guides the LLM to write structured feedback followed by the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation: Evaluating Generated Answers\n",
    "\n",
    "This function evaluates the quality of answers generated by the RAG system using a predefined evaluation prompt and a scoring language model. The evaluation process is iterative and updates the results file in place for checkpointing and saving progress.\n",
    "\n",
    "---\n",
    "\n",
    "#### What This Function Does:\n",
    "\n",
    "1. **Initialize the Evaluation Environment:**\n",
    "   - **`answer_path`:** Path to the JSON file containing the generated answers.\n",
    "   - **`eval_chat_model`:** The language model used for evaluation (e.g., GPT-4).\n",
    "   - **`evaluator_name`:** A string identifier for the evaluator (e.g., \"GPT4\").\n",
    "   - **`evaluation_prompt_template`:** The prompt template that defines how the evaluation task is framed.\n",
    "\n",
    "2. **Load Existing Results:**\n",
    "   - If the `answer_path` file exists, loads the previously saved results into `answers`.\n",
    "   - This ensures that previously evaluated answers are not re-evaluated, saving time and resources.\n",
    "\n",
    "3. **Iterate Over Generated Answers:**\n",
    "   - For each entry in `answers`:\n",
    "     - **Check for Prior Evaluation:** If the answer has already been evaluated by the specified evaluator (`eval_score_{evaluator_name}`), skip it.\n",
    "     - **Prepare the Evaluation Prompt:**\n",
    "       - Uses `evaluation_prompt_template` to format the instruction, response, and reference answer into the structured prompt.\n",
    "     - **Evaluate the Response:**\n",
    "       - Sends the prompt to the `eval_chat_model` (e.g., GPT-4) and receives the evaluation result.\n",
    "     - **Parse the Result:**\n",
    "       - Extracts `feedback` and `score` from the model's output, splitting on `[RESULT]` to ensure the expected format is followed.\n",
    "\n",
    "4. **Update the Results:**\n",
    "   - Adds the following fields to the current experiment:\n",
    "     - **`eval_score_{evaluator_name}`:** The numeric score assigned by the evaluator.\n",
    "     - **`eval_feedback_{evaluator_name}`:** The detailed feedback provided by the evaluator.\n",
    "   - Saves the updated `answers` list back to the `answer_path` file after each iteration for checkpointing.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "#### Example Usage:\n",
    "\n",
    "```python\n",
    "evaluate_answers(\n",
    "    answer_path=\"rag_test_results.json\",  # File containing generated answers\n",
    "    eval_chat_model=ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0),  # Evaluation model\n",
    "    evaluator_name=\"GPT4\",  # Identifier for the evaluator\n",
    "    evaluation_prompt_template=evaluation_prompt_template,  # Evaluation prompt template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "eval_chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
    "evaluator_name = \"GPT4\"\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm.tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        feedback, score = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation: Running the Complete RAG Evaluation Pipeline\n",
    "\n",
    "This script integrates all the steps covered so far to run a full evaluation of the RAG system across different configurations. It includes embedding creation, chunking, retrieval, generation, and evaluation in a loop to test multiple setups.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step-by-Step Breakdown:\n",
    "\n",
    "1. **Create Output Directory:**\n",
    "   - Ensures that a directory named `./output` exists to store the results.\n",
    "\n",
    "2. **Define Configurations:**\n",
    "   - **`embedding_models`:** List of embedding models to test (e.g., `\"sentence-transformers/all-mpnet-base-v2\"`).\n",
    "   - **`chunk_sizes`:** List of `(chunk_size, overlap)` tuples to test different chunking strategies.\n",
    "     - Example:\n",
    "       - `[2000, 100]`: Chunks of 2000 characters with 100-character overlap.\n",
    "       - `[5000, 500]`: Larger chunks of 5000 characters with 500-character overlap.\n",
    "\n",
    "3. **Iterate Over Configurations:**\n",
    "   - Loops through all combinations of `embedding_models` and `chunk_sizes`.\n",
    "   - Constructs a unique `settings_name` for each combination to name the output files clearly.\n",
    "\n",
    "4. **Build the Knowledge Base:**\n",
    "   - Calls the `build_rag_model` function with:\n",
    "     - `texts`: The input data (e.g., pre-split chunks of the documents).\n",
    "     - `embedding_model`: The current embedding model.\n",
    "     - `chunk_size` and `chunk_overlap`: Parameters for splitting the text.\n",
    "   - Converts the resulting vector store into a retriever (`knowledge_index`) for querying.\n",
    "\n",
    "5. **Run the RAG System:**\n",
    "   - Iterates through the `eval_dataset` (assumed to contain questions and true answers).\n",
    "   - Calls the `answer_with_rag` function to:\n",
    "     - Retrieve relevant documents using the `knowledge_index`.\n",
    "     - Generate answers using the RAG chain (`rag_chain`).\n",
    "   - Appends the results to a list, including:\n",
    "     - The question, true answer, generated answer, and retrieved documents.\n",
    "\n",
    "6. **Save Results:**\n",
    "   - Saves the answers to a JSON file named based on the configuration (`output_file_name`).\n",
    "\n",
    "7. **Evaluate the Answers:**\n",
    "   - Calls `evaluate_answers` to:\n",
    "     - Critique and score the generated answers using the LLM evaluator (`eval_chat_model`).\n",
    "     - Update the saved results with scores and feedback for each answer.\n",
    "\n",
    "---\n",
    "\n",
    "#### What This Script Accomplishes:\n",
    "\n",
    "1. **End-to-End Workflow:**\n",
    "   - Automates the entire RAG pipeline, from embedding creation to evaluation.\n",
    "\n",
    "2. **Flexible Testing:**\n",
    "   - Tests multiple configurations for embeddings and chunking, enabling comparative analysis.\n",
    "\n",
    "3. **Results Storage:**\n",
    "   - Saves intermediate and final results to disk for reproducibility and further analysis.\n",
    "\n",
    "4. **Scoring and Feedback:**\n",
    "   - Generates actionable feedback and numerical scores for the generated answers.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example Workflow:\n",
    "\n",
    "**Configuration 1:**\n",
    "- **Embedding Model:** `\"sentence-transformers/all-mpnet-base-v2\"`\n",
    "- **Chunk Size:** `2000`\n",
    "- **Overlap:** `100`\n",
    "\n",
    "**Sample Output File:**\n",
    "- `./output/rag_chunk:2000_embeddings:sentence-transformers~all-mpnet-base-v2.json`\n",
    "\n",
    "**Generated Results:**\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"question\": \"What are the symptoms of asthma?\",\n",
    "        \"generated_answer\": \"Asthma symptoms include shortness of breath and chest tightness.\",\n",
    "        \"true_answer\": \"Shortness of breath, wheezing, and chest tightness.\",\n",
    "        \"retrieved_docs\": [\"Document 1 text\", \"Document 2 text\"],\n",
    "        \"eval_score_GPT4\": \"4\",\n",
    "        \"eval_feedback_GPT4\": \"The response is mostly correct, but it omits 'wheezing' from the symptoms listed in the reference answer.\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "# Configurations\n",
    "embedding_models = [\"sentence-transformers/all-mpnet-base-v2\"]  # Add more models as needed\n",
    "chunk_sizes = [[2000,100], [5000,500]]  # Add more chunk sizes as needed\n",
    "\n",
    "# Iterate through configurations\n",
    "for chunk_size in chunk_sizes:\n",
    "    for embedding_model in embedding_models:\n",
    "        settings_name = f\"chunk:{chunk_size}_embeddings:{embedding_model.replace('/', '~')}\"\n",
    "        output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "\n",
    "        print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "        print(\"Loading knowledge base embeddings...\")\n",
    "        # Use rag_builder to create the vector store\n",
    "        vector_store = build_rag_model(\n",
    "            texts=chunks,  # Assuming `chunks` contains pre-split text data\n",
    "            embedding_model=embedding_model,\n",
    "            chunk_value=chunk_size\n",
    "        )\n",
    "        retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2})\n",
    "\n",
    "        print(\"Running RAG...\")\n",
    "        answers = []\n",
    "        for sample in tqdm.tqdm(eval_dataset):  # Assume eval_dataset is iterable\n",
    "            question = sample[\"question\"]\n",
    "            true_answer = sample[\"answer\"]\n",
    "\n",
    "            # Call the RAG function to get the generated answer\n",
    "            generated_answer, relevant_docs = answer_with_rag(\n",
    "                question=question,\n",
    "                rag_chain=rag_chain,  # Replace with your RAG chain\n",
    "                retriever=retriever,\n",
    "            )\n",
    "\n",
    "            answers.append({\n",
    "                \"question\": question,\n",
    "                \"generated_answer\": generated_answer,\n",
    "                \"true_answer\": true_answer,\n",
    "                \"relevant_docs\": relevant_docs,\n",
    "            })\n",
    "\n",
    "        # Save results to file\n",
    "        with open(output_file_name, \"w\") as f:\n",
    "            json.dump(answers, f)\n",
    "\n",
    "        print(\"Running evaluation...\")\n",
    "        evaluate_answers(\n",
    "            output_file_name,\n",
    "            eval_chat_model,\n",
    "            evaluator_name,\n",
    "            evaluation_prompt_template,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation: Aggregating and Normalizing Evaluation Results\n",
    "\n",
    "This code collects evaluation results from multiple JSON files, combines them into a single dataset, and normalizes the evaluation scores for further analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step-by-Step Breakdown:\n",
    "\n",
    "1. **Initialize an Empty List:**\n",
    "   - `outputs = []`: Prepares a list to store the results from all JSON files.\n",
    "\n",
    "2. **Load JSON Files:**\n",
    "   - `glob.glob(\"./output/*.json\")`: Finds all JSON files in the `./output` directory.\n",
    "   - For each file:\n",
    "     - Loads the JSON content into a pandas DataFrame using `pd.DataFrame`.\n",
    "     - Adds a new column, `settings`, to store the filename, indicating the configuration used for generating the results.\n",
    "     - Appends the DataFrame to the `outputs` list.\n",
    "\n",
    "3. **Combine All Results:**\n",
    "   - `pd.concat(outputs)`: Concatenates all DataFrames in the `outputs` list into a single DataFrame named `result`.\n",
    "\n",
    "4. **Normalize Evaluation Scores:**\n",
    "   - **Convert to Integer:**\n",
    "     - `result[\"eval_score_GPT4\"].apply(lambda x: int(x) if isinstance(x, str) else 1)`:\n",
    "       - Ensures all scores are integers, with a fallback value of `1` for non-numeric entries.\n",
    "   - **Normalize to Range [0, 1]:**\n",
    "     - `result[\"eval_score_GPT4\"] = (result[\"eval_score_GPT4\"] - 1) / 4`:\n",
    "       - Transforms the scores from the range `[1, 5]` to `[0, 1]`:\n",
    "         - Subtracts `1` to shift the range to `[0, 4]`.\n",
    "         - Divides by `4` to scale the range to `[0, 1]`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Purpose of This Step:\n",
    "\n",
    "1. **Aggregate Results:**\n",
    "   - Combines evaluation results from multiple configurations into a single dataset, making it easier to compare and analyze performance.\n",
    "\n",
    "2. **Normalize Scores:**\n",
    "   - Converts the raw scores into a standardized format (`[0, 1]`) for consistent interpretation and comparison across configurations.\n",
    "\n",
    "3. **Preserve Configuration Context:**\n",
    "   - Adds the `settings` column to retain information about which configuration each set of results corresponds to.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = []\n",
    "for file in glob.glob(\"./output/*.json\"):\n",
    "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "    output[\"settings\"] = file\n",
    "    outputs.append(output)\n",
    "result = pd.concat(outputs)\n",
    "result[\"eval_score_GPT4\"] = result[\"eval_score_GPT4\"].apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "result[\"eval_score_GPT4\"] = (result[\"eval_score_GPT4\"] - 1) / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation: Calculating and Sorting Average Scores by Configuration\n",
    "\n",
    "This code calculates the average evaluation scores for each configuration and sorts them in ascending order to identify the best and worst-performing setups.\n",
    "\n",
    "---\n",
    "\n",
    "#### Purpose of This Step:\n",
    "\n",
    "1. **Performance Comparison:**\n",
    "   - Calculates the overall effectiveness of each configuration by averaging the normalized evaluation scores across all questions.\n",
    "   - Highlights configurations that consistently produce better results.\n",
    "\n",
    "2. **Identify Trends:**\n",
    "   - Sorting the scores helps visualize how different configurations affect the system's performance.\n",
    "   - Useful for pinpointing the impact of factors like chunk size, overlap, or embedding model.\n",
    "\n",
    "\n",
    "3. **Insights into Configurations:**\n",
    "   - Identifies which configurations yield higher-quality answers, guiding optimization efforts.\n",
    "   - Helps determine the best chunk size, overlap, or embedding model for the\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores = result.groupby(\"settings\")[\"eval_score_GPT4\"].mean()\n",
    "average_scores.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
